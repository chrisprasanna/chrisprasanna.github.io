<!DOCTYPE HTML>
<!--
	Dopetrope by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Tetris-Playing Artificial Intelligence Agent - Chris Prasanna</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />

		<!-- Browser Icon -->
		<link rel="icon" href="icon7.ico" type="image/x-icon" />
		<link rel="shortcut icon" href="icon7.ico" type="image/x-icon" />

		<!-- Common Header and Footer -->
		<script
			src="https://code.jquery.com/jquery-3.3.1.js"
			integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
			crossorigin="anonymous">
		</script>
		<script> 
			$(function(){
			  $("#header").load("header.html"); 
			  $("#footer").load("footer.html"); 
			});
		</script> 
	</head>
	<body class="no-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
			<div id="header"></div>

			<!-- Main -->
				<section id="main">
					<div class="container">
						<!-- Content -->
							<article class="box post">

								<!-- <div class="slider-container">
									<span id="slider-image-1"></span>
									<span id="slider-image-2"></span>
									<div class="image-container">
										<img src="images/Tetris/agent.png" class="slide-image">
										<img src="images/Tetris/results.png" class="slide-image">
									</div>
									<div class="button-container">
										<a href="#slider-image-1" class="slider-button"></a>
										<a href="#slider-image-2" class="slider-button"></a>
									</div>
								</div> -->

								<div class="slideshow-container">

									<!-- Full-width images with number and caption text -->
									<div class="mySlides fade">
									  <div class="numbertext">1 / 2</div>
									  <img src="images/Tetris/agent.png" style="width:100%">
									  <div class="text"></div>
									</div>
								  
									<div class="mySlides fade">
									  <div class="numbertext">2 / 2</div>
									  <img src="images/Tetris/results.png" style="width:100%">
									  <div class="text"></div>
									</div>
								  
									<!-- Next and previous buttons -->
									<a class="prev" onclick="plusSlides(-1)">&#10094;</a>
									<a class="next" onclick="plusSlides(1)">&#10095;</a>
								  </div>
								  <br>
								  
								<!-- The dots/circles -->
								<div style="text-align:center">
									<span class="dot" onclick="currentSlide(1)"></span>
									<span class="dot" onclick="currentSlide(2)"></span>
								</div>

								<!-- <a href="#" class="image centered img"><img src="images/JR Dynamic.png" alt="" /></a> -->
								<header>
									<h2>Teaching AI to Play Tetris using Reinforcement Learning</h2>
									<p>Graduate Project</p>
								</header>
								<section>
									<header>
										<h3>Project Goal</h3>
									</header>
									<p>
										The goal of this project was to build a Tetris-playing AI agent that 
										improved with experience and cleared over 10,000 
										lines on average before losing. 
									</p>
								</section>
								<header>
									<h3>Challenges</h3>
								</header>
								<p>
									Choosing the features to include in the cost function was the most critical design choice
									for this project. This decision was important because 
									too few features can limit the performance of the agent while too many irrelevant 
									features can cause the agent to learn based off features that are not helpful to 
									the problem.
								</p>
								<p>
									State representation is also a challenge for designing artificial players.  On the 
									standard Tetris game, the total number of states is 10<sup>60</sup> which cannot be explored 
									directly. Therefore, approximation and learning techniques must be implemented. 
									The learning strategy must converge quickly and optimize the cost function based on 
									a combination of exploitation and exploration in order to achieve the best performance. 
								</p>
								<header>
									<h3>My Solution</h3>
								</header>
								<p>
									The reinforcement learning framework involved extracting relevant features from the environment, 
									using them to set up a linear cost function, simulating the cost for all possible actions, using 
									these costs to inform the decision-making process, and repeating until the agent loses the game. 
									During training, the top eight performing agents of a generation (training iteration) make up an elite set and move 
									on to the next generation. This elite set is then used to “reproduce” 32 “child” parameter sets. 
									These steps repeat for 20 generations and the best-performing agent in the final generation is 
									selected as the optimized parameter set.
								</p>
								<p>
									Features were chosen based on aspects of the game that the player will want to either minimize or maximize. 
									For instance, a successful player will likely try to maximize the number of cleared lines 
									while avoiding features such as holes, row transitions, and column transitions. Additionally, a linear cost function was 
									chosen due to its simple structure, which can reduce training time. The program only needs to 
									learn coefficients associated with each feature as opposed to polynomials, combination functions, or other 
									nonlinear functions.
								</p>
								<header>
									<h3>Notable Features & Accomplishments</h3>
								</header>
								<p>
									<ul>
										<li>Algorithms are implemented in an OpenAI Gym environment where 5 roll-outs are simulated for each parameter 
											set and the average reward is evaluated.</li>
										<li>One agent learned through a genetic algorithm which kept elite sets of reward function parameters while 
											producing other similar sets to use in the next generation.</li>
										<li>A second agent learned through the cross-entropy method which uses a sampling-based search for control 
											environments then updates the distribution based on which samples scored the highest.</li>
										<li>The optimized AI agent cleared an average of 88,563 lines and the maximum cleared lines in a single game 
											was 332,896. The Guinness Record for lines cleared by a human player is 4,988. </li>
									</ul>
								</p>
								<header>
									<h3>Skills Used</h3>
								</header>
								<p>
									<ul class="three-cols">
										<li>Python</li>
										<li>Reinforcement Learning</li>
										<li>Algorithm Implementation</li>
										<li>Black Box Optimization</li>
										<li>Genetic / Evolutionary Algorithms</li>
										<li>Feature Selection</li>
										<li>Feature Engineering</li>
										<li>Monte Carlo Methods</li>
										<li>Cross-Entropy Method</li>
										<li>OpenAI Gym</li>
										<li>Simulation</li>
									</ul>
								</p>
							</article>

					</div>
				</section>

			<!-- Footer -->
			<div id="footer"></div>

		</div>

		<!-- Back to Top Button --> 
		<a href="#" class="to-top">
			<i class="fas fa-chevron-up"></i>
		</a>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.dropotron.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<script src="assets/js/slideshow.js"></script>
		<script src="assets/js/back_to_top.js"></script>

	</body>
</html>
